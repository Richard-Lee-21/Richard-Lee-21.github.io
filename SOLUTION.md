# 解决方案说明 / Solution Explanation

## 问题 / Problem

**中文：**
GitHub Actions 中的爬虫可以定时运行，但页面不会更新。

**English:**
The crawler in GitHub Actions runs on schedule, but the pages don't get updated.

## 原因 / Root Cause

**中文：**
爬虫虽然成功执行并修改了文件，但这些更改只存在于工作流的临时环境中，没有被提交（commit）和推送（push）回代码仓库。因此，GitHub Pages 不知道有新内容需要重新构建。

**English:**
While the crawler successfully executes and modifies files, these changes only exist in the workflow's temporary environment. They are not committed and pushed back to the repository, so GitHub Pages doesn't know there's new content to rebuild.

## 解决方案 / Solution

### 关键修改 / Key Changes

1. **添加写入权限 / Add Write Permissions**
   ```yaml
   permissions:
     contents: write
   ```
   允许工作流推送更改到仓库 / Allows the workflow to push changes to the repository

2. **提交并推送更改 / Commit and Push Changes**
   ```bash
   git config --local user.email "github-actions[bot]@users.noreply.github.com"
   git config --local user.name "github-actions[bot]"
   git add -A
   git commit -m "chore: update crawled data [skip ci]"
   git push
   ```
   确保爬取的数据被保存到仓库 / Ensures crawled data is saved to the repository

### 工作流程 / Workflow

1. ⏰ 定时触发（每天午夜 UTC）/ Scheduled trigger (daily at midnight UTC)
2. 🤖 运行爬虫获取数据 / Run crawler to fetch data
3. 💾 提交更改到仓库 / Commit changes to repository
4. 📤 推送到 GitHub / Push to GitHub
5. 🔄 GitHub Pages 自动重建 / GitHub Pages automatically rebuilds
6. ✅ 页面显示最新数据 / Page displays latest data

## 使用方法 / Usage

### 测试 / Testing

1. 进入 Actions 标签页 / Go to Actions tab
2. 选择 "Scheduled Crawler" 工作流 / Select "Scheduled Crawler" workflow
3. 点击 "Run workflow" / Click "Run workflow"
4. 等待完成后检查仓库中的新文件 / After completion, check for new files in repository
5. GitHub Pages 将自动重建 / GitHub Pages will automatically rebuild

### 自定义 / Customization

编辑 `.github/workflows/crawler.yml` 文件，将示例爬虫替换为你的实际爬虫逻辑。

Edit `.github/workflows/crawler.yml` and replace the example crawler with your actual crawler logic.

## 重要提示 / Important Notes

- **`[skip ci]`**: 提交信息中的这个标记防止无限循环（如果你有其他 CI/CD 在推送时触发）
  This tag in the commit message prevents infinite loops (if you have other CI/CD that triggers on push)

- **更改检测**: 工作流会检查是否有更改，只在有更改时才提交，避免空提交
  Change detection: The workflow checks for changes and only commits when there are changes, avoiding empty commits

- **GitHub Pages 设置**: 确保你的 GitHub Pages 源设置为正确的分支（通常是 `main`）
  GitHub Pages settings: Make sure your GitHub Pages source is set to the correct branch (usually `main`)

## 文件说明 / Files

- `.github/workflows/crawler.yml` - 主工作流文件 / Main workflow file
- `.github/workflows/README.md` - 详细技术文档 / Detailed technical documentation  
- `index.html` - 示例页面，显示爬取的数据 / Example page displaying crawled data
- `data.json` - 爬虫生成的数据文件（首次运行后生成）/ Data file generated by crawler (created after first run)

## 故障排除 / Troubleshooting

### 问题：工作流运行但没有推送更改 / Issue: Workflow runs but doesn't push changes

**解决方案 / Solution:**
- 检查是否添加了 `permissions: contents: write`
  Check if `permissions: contents: write` is added
- 确认爬虫确实修改了文件
  Confirm the crawler actually modifies files

### 问题：GitHub Pages 没有更新 / Issue: GitHub Pages doesn't update

**解决方案 / Solution:**
- 检查 GitHub Pages 是否启用
  Check if GitHub Pages is enabled
- 确认 Pages 源分支设置正确
  Confirm Pages source branch is set correctly
- 查看 "pages build and deployment" 工作流是否成功运行
  Check if "pages build and deployment" workflow runs successfully
